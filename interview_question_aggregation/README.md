# ğŸš€ Interview Question Aggregation Pipeline

A **multi-source data ingestion pipeline** that scrapes interview questions from different websites, normalizes them, and stores them in PostgreSQL with deterministic deduplication.

The system is designed to be **modular, reproducible, and extensible**, following real-world data engineering practices instead of a basic scraper script.

---

# ğŸ§  Project Overview

This project aggregates interview questions and answers from multiple public websites and stores them in a structured PostgreSQL database.

Each website has its own scraper, while all data flows into a shared schema using **hash-based deduplication**.

## ğŸ¯ Key Goals

* Reliable scraping from heterogeneous HTML structures
* Clean separation of concerns (scraping â†’ transformation â†’ storage)
* Idempotent ingestion (safe to re-run)
* Future-ready design for AI enrichment

---

# ğŸ§± Architecture

```
Websites
   â”‚
   â”œâ”€â”€ Scraper (Site 1)
   â”œâ”€â”€ Scraper (Site 2)
   â”œâ”€â”€ Scraper (Site 3)
   â”‚
   â–¼
Normalized Records
   â”‚
   â–¼
PostgreSQL (core.questions)
```

## âš™ï¸ Design Principles

* One scraper per site (no universal scraper hacks)
* Database is the source of truth
* Scraping â‰  Enrichment
* Fail-safe parsing (ignore > misclassify)

---

# ğŸ“ Project Structure

```
interview_question_aggregation/
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ gfg_scraper.py
â”‚   â”œâ”€â”€ site2_scraper.py
â”‚   â”œâ”€â”€ site3_scraper.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ connection.py
â”‚   â”œâ”€â”€ insert_questions.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ id_generator.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â”œâ”€â”€ test_scraper_site2.py
â”‚   â””â”€â”€ test_scraper_site3.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# ğŸ—„ï¸ Database Schema

## Table: `core.questions`

| Column           | Type      | Description                         |
| ---------------- | --------- | ----------------------------------- |
| question_id      | TEXT (PK) | SHA-256 hash of normalized question |
| questions        | TEXT      | Interview question                  |
| answers          | TEXT      | Answer (nullable)                   |
| topics           | TEXT      | Topic / category                    |
| difficulty_level | TEXT      | Difficulty (nullable)               |
| is_ai_generated  | BOOLEAN   | Flag for AI-generated answers       |

---

# ğŸ” Deduplication Strategy

* `question_id` is generated using SHA-256 hash of normalized question text
* Inserts use **ON CONFLICT DO NOTHING**
* Running the pipeline multiple times does not create duplicates

---

# ğŸ”‘ Question ID Generation

```python
def generate_question_id(question_text):
    normalized = " ".join(question_text.lower().strip().split())
    return hashlib.sha256(normalized.encode("utf-8")).hexdigest()
```

## Why This Works

* Case insensitivity
* Whitespace normalization
* Deterministic uniqueness across sources

---

# ğŸ•·ï¸ Scraping Strategy

## ğŸ“Œ Common Rules

* Questions are treated as primary data
* Answers are optional
* Missing answers â†’ stored as `NULL`
* No fake placeholders like "Not available"

## ğŸŒ Site Handling

Each site has:

* Independent parsing logic
* Conservative classification rules
* Explicit stop conditions for answer extraction

This avoids:

* Topic leakage into difficulty
* Partial answers
* Structural assumptions

---

# â–¶ï¸ How to Run

## 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

## 2ï¸âƒ£ Configure PostgreSQL

Update credentials in:

```
database/connection.py
```

## 3ï¸âƒ£ Run the Pipeline

```bash
python -m scripts.run_pipeline
```

## âœ… Expected Output

```
Pipeline started
Site 1 records: XX
Site 2 records: XX
Site 3 records: XX
Pipeline finished
```

Re-running the command will not duplicate data.

---

# ğŸ§ª Testing

Each scraper can be tested independently:

```bash
python -m scripts.test_scraper
python -m scripts.test_scraper_site2
python -m scripts.test_scraper_site3
```

---

# ğŸ¤– AI Enrichment (Planned)

AI is treated as an enrichment layer, not a data source.

## Planned Behavior

* Only fill rows where `answers IS NULL`
* Never overwrite scraped answers
* Mark AI answers with `is_ai_generated = TRUE`
* Run as a separate script

---

# ğŸš« Why Selenium Is Not Used

Selenium was intentionally avoided because:

* It introduces unnecessary complexity
* It is slow and fragile for text extraction
* It breaks CLI and server-side automation

The project focuses on data pipelines, not browser automation.

---

# ğŸ“ˆ Future Improvements

* AI-based answer enrichment
* Topic normalization
* Difficulty inference
* Scheduled ingestion
* API layer for querying questions

---

# ğŸ§  Key Takeaways

* Robust scraping is about structure, not assumptions
* Ignoring ambiguous data is safer than misclassifying it
* Separation of concerns makes systems extensible
